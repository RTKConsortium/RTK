/*=========================================================================
 *
 *  Copyright RTK Consortium
 *
 *  Licensed under the Apache License, Version 2.0 (the "License");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *         https://www.apache.org/licenses/LICENSE-2.0.txt
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an "AS IS" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 *
 *=========================================================================*/

#ifndef rtkCudaUtilities_hcu
#define rtkCudaUtilities_hcu

#include <cuda_runtime_api.h>
#include <string>
#include <vector>
#define ITK_STATIC
#include <itkMacro.h>
#undef ITK_STATIC
#include <cuda.h>

#define CUDA_CHECK_ERROR                                                                   \
  {                                                                                        \
    cudaError_t err = cudaGetLastError();                                                  \
    if (cudaSuccess != err)                                                                \
      itkGenericExceptionMacro(<< "CUDA ERROR: " << cudaGetErrorString(err) << std::endl); \
  }

#define CUFFT_CHECK_ERROR(result)                                          \
  {                                                                        \
    if (result)                                                            \
      itkGenericExceptionMacro(<< "CUFFT ERROR #" << result << std::endl); \
  }

std::vector<int>
GetListOfCudaDevices();
std::pair<int, int>
GetCudaComputeCapability(int device);
size_t
GetFreeGPUGlobalMemory(int device);

inline __host__ __device__ float3
matrix_multiply(float3 a, float * matrix)
{
  return make_float3(matrix[0] * a.x + matrix[1] * a.y + matrix[2] * a.z + matrix[3],
                     matrix[4] * a.x + matrix[5] * a.y + matrix[6] * a.z + matrix[7],
                     matrix[8] * a.x + matrix[9] * a.y + matrix[10] * a.z + matrix[11]);
}
inline __host__ __device__ float3
operator-(float3 a, float3 b)
{
  return make_float3(a.x - b.x, a.y - b.y, a.z - b.z);
}
inline __host__ __device__ float3
fminf(float3 a, float3 b)
{
  return make_float3(fminf(a.x, b.x), fminf(a.y, b.y), fminf(a.z, b.z));
}
inline __host__ __device__ float3
fmaxf(float3 a, float3 b)
{
  return make_float3(fmaxf(a.x, b.x), fmaxf(a.y, b.y), fmaxf(a.z, b.z));
}
inline __host__ __device__ float
dot(float3 a, float3 b)
{
  return a.x * b.x + a.y * b.y + a.z * b.z;
}
inline __host__ __device__ float3
operator/(float3 a, float3 b)
{
  return make_float3(a.x / b.x, a.y / b.y, a.z / b.z);
}
inline __host__ __device__ float3
operator/(float3 a, float b)
{
  return make_float3(a.x / b, a.y / b, a.z / b);
}
inline __host__ __device__ float3
operator*(float3 a, float3 b)
{
  return make_float3(a.x * b.x, a.y * b.y, a.z * b.z);
}
inline __host__ __device__ float3
operator*(float b, float3 a)
{
  return make_float3(b * a.x, b * a.y, b * a.z);
}
inline __host__ __device__ float3
operator+(float3 a, float3 b)
{
  return make_float3(a.x + b.x, a.y + b.y, a.z + b.z);
}
inline __host__ __device__ float3
operator+(float3 a, float b)
{
  return make_float3(a.x + b, a.y + b, a.z + b);
}
inline __host__ __device__ void
operator+=(float3 & a, float3 b)
{
  a.x += b.x;
  a.y += b.y;
  a.z += b.z;
}
inline __host__ __device__ int
iDivUp(int a, int b)
{
  return (a % b != 0) ? (a / b + 1) : (a / b);
}
inline __host__ __device__ float
dot_vector(float3 u, float3 v)
{
  return u.x * v.x + u.y * v.y + u.z * v.z;
}

__host__ void
prepareScalarTextureObject(int                          size[3],
                           float *                      dev_ptr,
                           cudaArray *&                 threeDArray,
                           cudaTextureObject_t &        tex,
                           const bool                   isProjections,
                           const bool                   isLinear = true,
                           const cudaTextureAddressMode texAddressMode = cudaAddressModeBorder);

__host__ void
prepareVectorTextureObject(int                                size[3],
                           const float *                      dev_ptr,
                           std::vector<cudaArray *> &         componentArrays,
                           const unsigned int                 nComponents,
                           std::vector<cudaTextureObject_t> & tex,
                           const bool                         isProjections,
                           const cudaTextureAddressMode       texAddressMode = cudaAddressModeBorder);

__host__ void
prepareGeometryTextureObject(int                   size,
                             const float *         geometry,
                             float *&              dev_geom,
                             cudaTextureObject_t & tex_geom,
                             const unsigned int    nParam);

inline __device__ void
matrix_matrix_multiply(float * A, float * B, float * C, unsigned int rowsA, unsigned int colsB, unsigned int colsArowsB)
{
  unsigned int indexInA, indexInB, indexInC;
  for (unsigned int rowA = 0; rowA < rowsA; rowA++)
  {
    for (unsigned int colB = 0; colB < colsB; colB++)
    {
      indexInC = rowA * colsB + colB;
      C[indexInC] = 0.0f;
      for (unsigned int colArowB = 0; colArowB < colsArowsB; colArowB++)
      {
        indexInA = rowA * colsArowsB + colArowB;
        indexInB = colArowB * colsB + colB;
        C[indexInC] += A[indexInA] * B[indexInB];
      }
    }
  }
}

#endif
